{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "1. Overview of different ways to get data \n",
    "2. API: description, demo, practice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ways to get data from the web\n",
    "\n",
    "1. **Manually click and download.** The way you would have done it before this class.\n",
    "1. **Let `pandas` download.** E.g. our assignments often begin with `pd.read_stata(<url>)`. \n",
    "    - Did you know? Pandas can often directly read tables on webpages! Try `pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')`\n",
    "    - `read_html` can only handle basic HTML tables encoded directly in the page (no Javascript, e.g.) and only grabs displayed text -- no URLS.\n",
    "    - You can use the data without saving it to your hard drive if you want. The good/bad part of this is that if the website changes the data, the next time you run it, you'll get the newer version of data. (Unstable, potentially, but also updates automatically.)\n",
    "2. **\"Install and play\" APIs**, which let you interact with a website without specifying the exact API requests. API stands for Application Programming Interface, and it is a way for your computer to send a request (a query) to a server and get some response (hopefully useful data).\n",
    "    - The `pandas_datareader` plug in for Yahoo stock prices is one version of this. \n",
    "    - `datadotworld` was another. \n",
    "    - Kaggle and most of the [data sources listed on our main site](https://ledatascifi.github.io/studentresourcevert/resource-landing.html#getting-data-and-practicing-ml) have API packages for Python. \n",
    "    - I download your peer reviews using `PyGithub`. \n",
    "4. **Manual API queries** for websites without \"install and play\" APIs. Many sites have an API port of some kind serving up the data they show visitors. \n",
    "5. **Scraping the data implicitly on the website.** The last resort. You can't find the API serving the data, but your eyes see it. And you want it, cause websites contain a lot of data, like [GoT's IMDB page](https://www.imdb.com/title/tt0944947/?ref_=nv_sr_srsg_0).\n",
    "    - AGAIN: The last resort.\n",
    "    - Wisdom from [Greg Reda](http://www.gregreda.com/2013/03/03/web-scraping-101-with-python/):\n",
    "    \n",
    "> 1. You should check a site's terms and conditions before you scrape them. It's their data and they likely have some rules to govern it.\n",
    "> 2.  Be nice - A computer will send web requests much quicker than a user can. Make sure you space out your requests a bit so that you don't hammer the site's server.\n",
    "> 3. Scrapers break - Sites change their layout all the time. If that happens, be prepared to rewrite your code.\n",
    "> 4. Web pages are inconsistent - There's sometimes some manual clean up that has to happen even after you've gotten your data.\n",
    "\n",
    "\n",
    "\n",
    "## Which should you choose?\n",
    "\n",
    "**Options 1-3 are BY FAR the easiest.** If you want more than 10 tables or so (but the threshold depends on your coding speed), I'd abandon the manual option and go with `pandas` or a nice API package. \n",
    "\n",
    "Never ever try \\#4 or \\#5 without searching for \"\\<website\\> python api\" first.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful packages, tricks, and tips\n",
    "\n",
    "- `urllib`, `urllib2`, and `requests` to open up webpages. `requests` is probably the best for sending API queries.\n",
    "- `beautifulsoup` is a good classic option for parsing web documents, as is `lxml` and `pyquery`\n",
    "- `requests_html` seems to be a new and excellent package (it purports to replace the `requests`/`beautifulsoup` combination I've used for five years)\n",
    "- `selenium` is one way to \"impersonate\" a human, and also can help develop scraping macros, but you might not need it except on difficult scraping projects. It opens a literal browser window. \n",
    "- `from time import sleep` will help you slow your code down so you don't get blocked\n",
    "- `from IPython.display import HTML` then `HTML(<html object>)` will show you what the HTML you have looks like. E.g. if you're using `r = requests(url)`, then you can use `HTML(r.text)` to see the request object. \n",
    "- If you have to scrap, you'll need to systematically \"find parts of the webpage\". E.g. Q: Where is that table? A: Oh, it's inside the HTML tag called \"table3\". \n",
    "    - How do you figure that out? Right click on an element you're interested and click \"Inspect Element\". (F12 is the Windows shortcut.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual APIs\n",
    "\n",
    "1. Prof led: [The Neal Caren \"First API\" example](https://nbviewer.jupyter.org/github/nealcaren/ScrapingData/blob/master/Notebooks/5_APIs.ipynb)\n",
    "2. For your reference: Sometimes a webpage is \"hiding\" an API. You run a search and the URL doesn't look obviously like a search. But often, inside that page is a \"backdoor\" to an API you can search just like the above example. [This](https://nbviewer.jupyter.org/github/nealcaren/ScrapingData/blob/master/Notebooks/Bonus_Undocument_APIs.ipynb) tutorial shows one example of this and more importantly, how the author found the API. \n",
    "3. YOUR TURN: Exchange rates\n",
    "    - Start with `url = 'https://api.exchangeratesapi.io/latest?base=NOK' `\n",
    "    - Q1: What is the average exchange rate value this search returns?\n",
    "    - [The documentation](https://exchangeratesapi.io/) for this database (how to search it, change parameters, etc.)\n",
    "    - Q2: Change the base to Euros, then tell me how many Japanese Yen is in a euro.\n",
    "    - Q3: What was the number of Yen per Euro on Jan 2, 2018?\n",
    "    - Q4: Bonus, prof can show: Get a time series of EURJPY from 2018 through 2019.\n",
    "    \n",
    "Now that you can write a `request` and modify search parameters, you might need to loop over the results.\n",
    "1. maybe you want to loop over the search term\n",
    "2. maybe you want to loop through pages of search results\n",
    "\n",
    "Here is a way you could do the first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_itunes(search_term):\n",
    "    '''Simplified iTunes search'''\n",
    "    \n",
    "    base_url = 'https://itunes.apple.com/search'\n",
    "    search_parameters = {'term': search_term}\n",
    "    \n",
    "    r = requests.get(base_url, params = search_parameters)\n",
    "    \n",
    "    results_df = pd.DataFrame(r.json()['results'])\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "search_itunes('billie eilish')      # one at a time\n",
    "search_itunes('father john misty')) # another\n",
    "\n",
    "artists = ['billie eilish','father john misty'] # or loop over them!\n",
    "for artist in artists:\n",
    "    df = search_itunes(artist)\n",
    "    # you could do anything with the results here\n",
    "    # maybe you just save the webpage/search results\n",
    "    # but this is just a toy illustration, so nothing\n",
    "    print(len(df)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "\n",
    "- STAT545, as always\n",
    "- [Neal Caren](https://nealcaren.org/)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
